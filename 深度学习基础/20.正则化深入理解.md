## 为什么需要正则化

深度学习可能存在过拟合问题，有两个解决方法：1.正则化  2.准备更多的数据 我们无法时刻准备充足的样本数据，因为获取数据本身成本就很高，但正则化技术通常有助于避免过拟合或减少网络参数。

## 常见的正则化技术

常见的机器学习算法的正则化方法有：L1正则化、L2正则化、Dropout、Early Stop、归一化（BN、IN、GN、LN）等

### L1和L2正则化

**定义**

正则化就是在原来损失函数的基础上加了一些正则化项（也叫模型复杂度惩罚项）。即目标函数变成了**原始损失函数+额外项**

L1正则化和L2正则化可以看做是**损失函数的惩罚项**。所谓**惩罚**是指对损失函数中的**某些参数做一些限制**。对于线性回归模型，**使用L1正则化的模型叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）**。

线性回归L1正则化损失函数：

![image-20221101103546867](C:\Users\10428\AppData\Roaming\Typora\typora-user-images\image-20221101103546867.png)

线性回归L2正则化损失函数：

![image-20221101103632992](C:\Users\10428\AppData\Roaming\Typora\typora-user-images\image-20221101103632992.png)

- 公式(1)(2)中ww表示特征的系数（xx的参数），可以看到正则化项是对系数做了限制。L1正则化和L2正则化的说明如下：
  - L1正则化是指权值向量ww中各个元素的绝对值之和，通常表示为∥w∥1‖w‖1。
  - L2正则化是指权值向量ww中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∥w∥22‖w‖22。
  - 一般都会在正则化项之前添加一个系数λλ。Python中用α表示，这个系数需要用户指定（也就是我们要调的超参）。

**作用**

- **L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。**

  **稀疏性**，说白了就是模型的很多参数是0。通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，很多参数是0，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，即使去掉对模型也没有什么影响，此时我们就可以只关注系数是非零值的特征。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。

- **L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。**

  **拟合过程中通常都倾向于让权值尽可能小**，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。**可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响**，专业一点的说法是**抗扰动能力强**。

**L1, L2参数如何选择**

以L2正则化参数为例：从公式(8)可以看到，λ越大，θjθj衰减得越快。另一个理解可以参考L2求解图， **λλ越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小**；当然也不是越大越好，太大容易引起欠拟合。

从0开始，逐渐增大λλ。在训练集上学习到参数，然后在测试集上验证误差。反复进行这个过程，直到测试集上的误差最小。一般的说，随着λλ从0开始增大，测试集的误分类率应该是先减小后增大，交叉验证的目的，就是为了找到误分类率最小的那个位置。建议一开始将正则项系数λ设置为0，先确定一个比较好的learning rate。然后固定该learning rate，给λλ一个值（比如1.0），然后根据validation accuracy，将λ增大或者减小10倍，增减10倍是粗调节，当你确定了λλ的合适的数量级后，比如λ=0.01λ=0.01，再进一步地细调节，比如调节为0.02，0.03，0.009之类。

## 参考

[深入理解L1、L2正则化](https://www.cnblogs.com/zingp/p/10375691.html)