

# 算法笔记

## Motivation

此帖子用来记录 CV基础知识，对知识点的自我理解，采用问答形式去掌握核心

## 检测基础

### 1. 如何计算mAP?

<u>原理：掌握   代码：了解伪代码</u>

> **问题：**
>
> 1)   AP及mAP 计算过程？
>
> 2）PR曲线如何绘制的，怎么反应模型性能好坏的，怎么找到二者的平衡？
>
> 3)  为何PR曲线中precision 与recall成负相关？

#### **基础概念**

**TP**：正确的检测的数量：正确分类且为正样本/前景（IOU >= threshold)

**FP**：错误的检测的数量：IOU <= threshold

**FN**: 漏检的数量

**TN**: 没有意义。因为我们只关注前景，不会去检测背景的框，没事找事啊

**Precision**: 反应模型找到正确检测框的能力
$$
precision = TP/(TP + FP) = TP/all-detections
$$
**Recall**:反应模型找到所有真实框的能力
$$
Recall = TP/(TP+FN) = TP/all-ground-truths
$$
**AP**: 平均精度，在不同recall下的最高precision的均值(一般会对各类别分别计算各自的AP)。

**mAP**:  平均精度的均值，各类别的AP的均值。

#### AP及mAP计算过程

ap是针对于一个类别，所以以下举其中一类的例子

**第一步：排序**

我们通过inference获得一批检测框(coordinate, score), 数量记作，然后按照类别划分，我们拿出其中一类的检测框，数量记作**n_dets**, 然后根据每个框的socre得分，降序排列，这样就得到一个序列（n_dets, score)。

**第二步：由IOU求TP、FP**

将每个检测框与GT做IOU计算，大于阈值即为TP，小于则为FP，此时已经可以计算出一组precision和recall

**第三步：动态计算precision&recall**

为啥说动态，因为我们调整score的阈值，从小到大依次累加，会影响检测框的总数，所以会得到不同组的precision和recall

这也是我们画PR曲线的基础

第四步：计算AP

voc07:如果按照[VOC07](https://arleyzhang.github.io/articles/1dc20586/)的AP计算标准，对于0.0-1.0的11个Recall阈值(间隔0.1），分别取每个阈值之上的最大Precision进行计算平均值(**11-point interpolation**)

![img](https://camo.githubusercontent.com/dfe080e1379c29387ed57b9ac23e72001a94f0e5b8eb02c69f1d6d3b2161e663/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f2535437465787425374241502537442533442535436672616325374231253744253742313125374425323025354373756d5f25374272253543696e2532302535436c65667425323025354325374225323030253243253230302e312532432532302e2e2e25324331253230253543726967687425323025354325374425374425354372686f5f25374225354374657874253742696e746572702537442535436c656674253230253238253230722532302535437269676874253230253239253744)

voc12: AP等于PR曲线的面积（ **interpolating all points**）

#### PR曲线绘制

由上面动态计算得到的Recall & Precision, 我们对应到（x,y)即可绘制出PR曲线。

**PR曲线特点**：

- recall 越小， precision反而越大，二者负相关

  因为如果把score阈值值很高，检测框数量就少(大量漏检），即算precison的分母很小，同时导致TP变小，自然结果值就大。但recall分母是GT的数量，这个是不变的，只有分子(TP)数量提高才能变大。

- 一般来说，我们希望上述两个指标都是越高越好。比较不同检测器效果，一般通过其面积来看，直观点就是如果一个学习器的P-R被另一个学习器的该曲线包围，则可以断言后面的要好些，如果有交叉点，我们就可以通过平衡点(准确率=召回率)，此图说明性能A>B>c

  <img src="https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/20170826114737648.png" alt="img" style="zoom:67%;" />

### 2.如何计算卷积参数量和计算量

https://www.jianshu.com/p/c2a0ba5bb3d1

https://zhuanlan.zhihu.com/p/395354063

https://blog.csdn.net/Brikie/article/details/112646865  归纳了计算通式，比较简洁

https://zhuanlan.zhihu.com/p/92134485 图解正常卷积和分离卷积，非常直观

#### 正常卷积

**参数量**：从卷积核数量角度去想

**通式**: 参数量 = 每个卷积核的参数 x 核的数量(输出通道数) + 偏置(输出通道数）

用于衡量模型大小
$$
params = (k_{w}*k_{h}*C_{in} + 1(bias))*C_{out}
$$
**计算量(FlOPs**)：注意s小写  从输出特征的pixel计算量角度去想, 要区别与FLOPS。

**通式：**Flops =  计算每个输出特征值的 乘法次数 + 加法次数 + 通道合并次数(忽略) + 偏置(输出通道数)

可以用来衡量算法/模型的复杂度

对于一个输出为WxHxC的Featture map，我们可以先计算得到一个通道上一个点(pixel)所用的计算量,再计算特征图所有点的
$$
乘法运算数M=k_{w}*k_h*C_{in} \\
加法运算数A=（k_{w}*k_h-1）*C_{in} \\
通道运算数C=C_{in} - 1 \\  
则 Flops= (M+A+C+1(bisa))*W_{out}*H_{out}*C_{out}\\
化简得到：Flops = (2*k_w*k_h*C_{in})*W_{out}*H_{out}*C_{out}
$$

#### 可分离卷积

可分离的卷积具体操作是先对输入map每单个channel进行卷积的操作(分组卷积)，然后再进行1x1卷积实现输出通道的改变(1*1全卷积)。

![微信截图_20220707161709](https://raw.githubusercontent.com/kongyan66/Img-for-md/master/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20220707161709.png)

**参数量：**

分两部分计算，每层单独卷积和1维卷积
$$
params = k_w*k_h*C_{in} + (1*1*C_{in}+1(bias))*C_{out}
$$
**计算量**

同上也分两部分，计算量计算遵循正常卷积计算公式来


$$
FLOPs = (2*k_w*k_h*1) * W_{out}*H_{out}*C_{in} + (2*1*1*C_{in})*W_{out}*H_{out}*C_{out}\\
化简的：FLOPs = 2*(K_w*K_h + C_{out})*W_{out}*H_{out}*C_{in}
$$
**结论**：对比发现，采用分离卷积，得到同样的Featture map, 参数量和计算量都大大减小了，这样可以设计出更深的网络。



## DL基础

### 1. 前向传播与反向传播计算

https://www.jianshu.com/p/964345dddb70

